{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Multi-Class Classification**\n",
        "\n",
        "THis is a classification problem, where the number of classes are more than 2. We use one-hot encoding for the response. The most common cost function is called the \"categorical cross-entropy\"."
      ],
      "metadata": {
        "id": "OAl8a7dlabmo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73gWwHNzaZGY"
      },
      "outputs": [],
      "source": [
        "## categorical cross entropy\n",
        "## each class has a prob and true y\n",
        "## say we need to predict y = {cat, dog, bird}\n",
        "import numpy as np\n",
        "def categorical_crossentropy(y1, y2, y3, p1, p2, p3):\n",
        "  loss = -(y1*np.log(p1) + y2*np.log(p2) + y3*np.log(p3))\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## I provide an image of a dog\n",
        "## y = [1,0,0]\n",
        "## p = [0.1, 0.8, 0.1]\n",
        "## yhat = argmax(p) [0,1,0]\n",
        "y1 = 0\n",
        "y2 = 1\n",
        "y3 = 0\n",
        "p1 = 0.05\n",
        "p2 = 0.9\n",
        "p3 = 0.05\n",
        "\n",
        "categorical_crossentropy(y1, y2, y3, p1, p2, p3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCY4j4QHbeIL",
        "outputId": "b530e9ea-1d49-4f04-b497-69970cfa9016"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.10536051565782628"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## vector-based\n",
        "## y is a vector\n",
        "## p is a vector\n",
        "def categorical_crossentropy(y, p):\n",
        "  loss = -(y[0]*np.log(p[0]) + y[1]*np.log(p[1]) + y[2]*np.log(p[2]))\n",
        "  return loss"
      ],
      "metadata": {
        "id": "cCJDod3fcJlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.array([1,0,0])\n",
        "p = np.array([0.9, 0.05, 0.05])"
      ],
      "metadata": {
        "id": "fd0rRyDwR7BG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_crossentropy(y, p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qZ4Cji5SGQ7",
        "outputId": "37390600-10b9-402b-d36a-75f86091bf5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.10536051565782628"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **IRIS Data set**\n",
        "\n",
        "Goal: predict whether the flower is a {setosa, versicolor, virginica}"
      ],
      "metadata": {
        "id": "fKssXiFbf-5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.datasets as datasets\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "A963AguidFzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = datasets.load_iris(return_X_y = True)"
      ],
      "metadata": {
        "id": "oiTYf6TVgZAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvcyofOWgjyc",
        "outputId": "5728ff5e-5105-4333-a64d-c276c7298452"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Logistic Regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "## One vs Rest Scheme\n",
        "lr = LogisticRegression(multi_class=\"auto\", max_iter = 1000)\n",
        "lr.fit(X, y)  ## one-hot encoding for y is done in the background"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "OU-lMISGgovz",
        "outputId": "50457b20-0df3-4016-ce39-f57c20826b84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(max_iter=1000)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Probabilities\n",
        "## vector of probabilities for each obs (matrix) n x 3\n",
        "np.round(lr.predict_proba(X),2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsCnGYKwg4rB",
        "outputId": "e5db2252-0593-4503-a1d2-b35ea0d96e90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.98, 0.02, 0.  ],\n",
              "       [0.97, 0.03, 0.  ],\n",
              "       [0.99, 0.01, 0.  ],\n",
              "       [0.98, 0.02, 0.  ],\n",
              "       [0.99, 0.01, 0.  ],\n",
              "       [0.97, 0.03, 0.  ],\n",
              "       [0.99, 0.01, 0.  ],\n",
              "       [0.98, 0.02, 0.  ],\n",
              "       [0.98, 0.02, 0.  ],\n",
              "       [0.97, 0.03, 0.  ],\n",
              "       [0.98, 0.02, 0.  ],\n",
              "       [0.98, 0.02, 0.  ],\n",
              "       [0.97, 0.03, 0.  ],\n",
              "       [0.99, 0.01, 0.  ],\n",
              "       [0.99, 0.01, 0.  ],\n",
              "       [0.99, 0.01, 0.  ],\n",
              "       [0.99, 0.01, 0.  ],\n",
              "       [0.98, 0.02, 0.  ],\n",
              "       [0.96, 0.04, 0.  ],\n",
              "       [0.98, 0.02, 0.  ],\n",
              "       [0.95, 0.05, 0.  ],\n",
              "       [0.98, 0.02, 0.  ],\n",
              "       [1.  , 0.  , 0.  ],\n",
              "       [0.95, 0.05, 0.  ],\n",
              "       [0.95, 0.05, 0.  ],\n",
              "       [0.95, 0.05, 0.  ],\n",
              "       [0.97, 0.03, 0.  ],\n",
              "       [0.97, 0.03, 0.  ],\n",
              "       [0.98, 0.02, 0.  ],\n",
              "       [0.97, 0.03, 0.  ],\n",
              "       [0.96, 0.04, 0.  ],\n",
              "       [0.96, 0.04, 0.  ],\n",
              "       [0.99, 0.01, 0.  ],\n",
              "       [0.99, 0.01, 0.  ],\n",
              "       [0.97, 0.03, 0.  ],\n",
              "       [0.98, 0.02, 0.  ],\n",
              "       [0.98, 0.02, 0.  ],\n",
              "       [0.99, 0.01, 0.  ],\n",
              "       [0.99, 0.01, 0.  ],\n",
              "       [0.97, 0.03, 0.  ],\n",
              "       [0.99, 0.01, 0.  ],\n",
              "       [0.96, 0.04, 0.  ],\n",
              "       [0.99, 0.01, 0.  ],\n",
              "       [0.97, 0.03, 0.  ],\n",
              "       [0.96, 0.04, 0.  ],\n",
              "       [0.97, 0.03, 0.  ],\n",
              "       [0.98, 0.02, 0.  ],\n",
              "       [0.98, 0.02, 0.  ],\n",
              "       [0.98, 0.02, 0.  ],\n",
              "       [0.98, 0.02, 0.  ],\n",
              "       [0.  , 0.87, 0.12],\n",
              "       [0.01, 0.86, 0.13],\n",
              "       [0.  , 0.73, 0.27],\n",
              "       [0.02, 0.94, 0.04],\n",
              "       [0.  , 0.82, 0.18],\n",
              "       [0.01, 0.86, 0.13],\n",
              "       [0.  , 0.72, 0.28],\n",
              "       [0.15, 0.85, 0.  ],\n",
              "       [0.  , 0.9 , 0.1 ],\n",
              "       [0.04, 0.91, 0.05],\n",
              "       [0.06, 0.94, 0.01],\n",
              "       [0.02, 0.9 , 0.09],\n",
              "       [0.01, 0.98, 0.01],\n",
              "       [0.  , 0.78, 0.22],\n",
              "       [0.07, 0.92, 0.01],\n",
              "       [0.01, 0.93, 0.07],\n",
              "       [0.01, 0.77, 0.22],\n",
              "       [0.02, 0.97, 0.02],\n",
              "       [0.  , 0.8 , 0.2 ],\n",
              "       [0.02, 0.96, 0.02],\n",
              "       [0.  , 0.44, 0.56],\n",
              "       [0.02, 0.96, 0.03],\n",
              "       [0.  , 0.6 , 0.4 ],\n",
              "       [0.  , 0.86, 0.14],\n",
              "       [0.01, 0.94, 0.05],\n",
              "       [0.01, 0.92, 0.07],\n",
              "       [0.  , 0.8 , 0.2 ],\n",
              "       [0.  , 0.48, 0.52],\n",
              "       [0.01, 0.81, 0.18],\n",
              "       [0.06, 0.93, 0.  ],\n",
              "       [0.03, 0.96, 0.01],\n",
              "       [0.04, 0.96, 0.01],\n",
              "       [0.03, 0.96, 0.02],\n",
              "       [0.  , 0.35, 0.65],\n",
              "       [0.01, 0.75, 0.24],\n",
              "       [0.01, 0.79, 0.2 ],\n",
              "       [0.  , 0.81, 0.19],\n",
              "       [0.  , 0.91, 0.08],\n",
              "       [0.03, 0.93, 0.04],\n",
              "       [0.02, 0.94, 0.04],\n",
              "       [0.01, 0.9 , 0.09],\n",
              "       [0.  , 0.83, 0.17],\n",
              "       [0.02, 0.96, 0.03],\n",
              "       [0.12, 0.88, 0.  ],\n",
              "       [0.01, 0.92, 0.07],\n",
              "       [0.02, 0.94, 0.04],\n",
              "       [0.02, 0.93, 0.06],\n",
              "       [0.01, 0.94, 0.06],\n",
              "       [0.24, 0.76, 0.  ],\n",
              "       [0.02, 0.94, 0.04],\n",
              "       [0.  , 0.  , 1.  ],\n",
              "       [0.  , 0.16, 0.84],\n",
              "       [0.  , 0.03, 0.97],\n",
              "       [0.  , 0.08, 0.92],\n",
              "       [0.  , 0.02, 0.98],\n",
              "       [0.  , 0.  , 1.  ],\n",
              "       [0.01, 0.51, 0.48],\n",
              "       [0.  , 0.02, 0.98],\n",
              "       [0.  , 0.05, 0.95],\n",
              "       [0.  , 0.01, 0.99],\n",
              "       [0.  , 0.21, 0.79],\n",
              "       [0.  , 0.14, 0.86],\n",
              "       [0.  , 0.07, 0.93],\n",
              "       [0.  , 0.15, 0.85],\n",
              "       [0.  , 0.04, 0.96],\n",
              "       [0.  , 0.05, 0.95],\n",
              "       [0.  , 0.12, 0.88],\n",
              "       [0.  , 0.  , 1.  ],\n",
              "       [0.  , 0.  , 1.  ],\n",
              "       [0.  , 0.45, 0.55],\n",
              "       [0.  , 0.02, 0.98],\n",
              "       [0.  , 0.19, 0.81],\n",
              "       [0.  , 0.  , 1.  ],\n",
              "       [0.  , 0.39, 0.61],\n",
              "       [0.  , 0.04, 0.96],\n",
              "       [0.  , 0.05, 0.95],\n",
              "       [0.  , 0.46, 0.54],\n",
              "       [0.  , 0.39, 0.61],\n",
              "       [0.  , 0.04, 0.96],\n",
              "       [0.  , 0.14, 0.86],\n",
              "       [0.  , 0.03, 0.97],\n",
              "       [0.  , 0.02, 0.98],\n",
              "       [0.  , 0.03, 0.97],\n",
              "       [0.  , 0.48, 0.52],\n",
              "       [0.  , 0.19, 0.81],\n",
              "       [0.  , 0.01, 0.99],\n",
              "       [0.  , 0.02, 0.98],\n",
              "       [0.  , 0.12, 0.88],\n",
              "       [0.  , 0.44, 0.56],\n",
              "       [0.  , 0.09, 0.91],\n",
              "       [0.  , 0.02, 0.98],\n",
              "       [0.  , 0.12, 0.88],\n",
              "       [0.  , 0.16, 0.84],\n",
              "       [0.  , 0.01, 0.99],\n",
              "       [0.  , 0.01, 0.99],\n",
              "       [0.  , 0.08, 0.92],\n",
              "       [0.  , 0.25, 0.75],\n",
              "       [0.  , 0.16, 0.84],\n",
              "       [0.  , 0.04, 0.96],\n",
              "       [0.  , 0.23, 0.76]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## the argmax function is used to predict\n",
        "## argmax assigns the prediction to the highest probability\n",
        "## there is NO cutoff\n",
        "## p = [0.2, 0.4, 0.4] done randomly then\n",
        "np.argmax(lr.predict_proba(X), axis = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FscWq7WhuXy",
        "outputId": "26bdb641-b5f9-45c4-c32e-3fff1952d488"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## predict function does np.argmax in the background\n",
        "lr.predict(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQQ2EsNdiOJ5",
        "outputId": "54b82ae6-4970-4b64-a15c-876ac50bdde4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Neural Networks**\n",
        "\n",
        "Tensorflow requires that we have a one-hot encoded y"
      ],
      "metadata": {
        "id": "MgwxqOtKiyiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "oc = OneHotEncoder()\n",
        "y_onehot = oc.fit_transform(y[:, np.newaxis]).toarray()"
      ],
      "metadata": {
        "id": "aEoa8kl-iqQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_onehot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mcgye0D1jEve",
        "outputId": "9224639f-853a-4960-a7db-84fdb96542b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **AdaBoost**"
      ],
      "metadata": {
        "id": "SHDGnI_YjeT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "ada = AdaBoostClassifier()\n",
        "ada.fit(X, y)\n",
        "yhat = ada.predict(X)"
      ],
      "metadata": {
        "id": "FE_ROO54jMuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yhat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-rVR5Wxjp4T",
        "outputId": "4ad9e9fd-fa2b-482c-8391-9f2c1185f6e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2,\n",
              "       2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tensorflow NN**"
      ],
      "metadata": {
        "id": "7WANBf2Xjx2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input\n",
        "\n",
        "nn = Sequential()\n",
        "nn.add(Input(shape = X.shape[1]))\n",
        "nn.add(Dense(units = 10, activation = \"tanh\"))\n",
        "nn.add(Dropout(0.05))\n",
        "\n",
        "## output layer has to have # number of class as units\n",
        "## activation function has to match multi-class\n",
        "nn.add(Dense(units = 3, activation = \"softmax\"))"
      ],
      "metadata": {
        "id": "8xGBjjOjjqj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## number of parameters\n",
        "nn.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JatRRaDtkrGE",
        "outputId": "351f9f87-760a-42b1-9537-c5a5599e24f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_5 (Dense)             (None, 10)                50        \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 10)                0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 3)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 83 (332.00 Byte)\n",
            "Trainable params: 83 (332.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Compile\n",
        "## multi-class - categorical_crossentropy\n",
        "nn.compile(optimizer = \"adam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])"
      ],
      "metadata": {
        "id": "n8mjP8pikuf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn_fit = nn.fit(X, y_onehot, batch_size = 20, epochs = 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AqHZJDDkx_E",
        "outputId": "5b4d096a-ef6b-44e1-c29e-733ebb6f88e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "8/8 [==============================] - 1s 3ms/step - loss: 1.4719 - accuracy: 0.3333\n",
            "Epoch 2/20\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 1.3432 - accuracy: 0.3400\n",
            "Epoch 3/20\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 1.2275 - accuracy: 0.3067\n",
            "Epoch 4/20\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 1.1429 - accuracy: 0.3200\n",
            "Epoch 5/20\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 1.0447 - accuracy: 0.3733\n",
            "Epoch 6/20\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 1.0102 - accuracy: 0.3600\n",
            "Epoch 7/20\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 1.0214 - accuracy: 0.4000\n",
            "Epoch 8/20\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.9701 - accuracy: 0.5467\n",
            "Epoch 9/20\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.9393 - accuracy: 0.5267\n",
            "Epoch 10/20\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.8923 - accuracy: 0.6333\n",
            "Epoch 11/20\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.8974 - accuracy: 0.5933\n",
            "Epoch 12/20\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.8537 - accuracy: 0.6467\n",
            "Epoch 13/20\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.8269 - accuracy: 0.6667\n",
            "Epoch 14/20\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.8102 - accuracy: 0.6867\n",
            "Epoch 15/20\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.8242 - accuracy: 0.6600\n",
            "Epoch 16/20\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.8114 - accuracy: 0.6267\n",
            "Epoch 17/20\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.7782 - accuracy: 0.6800\n",
            "Epoch 18/20\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.7255 - accuracy: 0.6733\n",
            "Epoch 19/20\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.7183 - accuracy: 0.6533\n",
            "Epoch 20/20\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.7097 - accuracy: 0.7400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Prediction\n",
        "nn.predict(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5-PJ2vzlOTu",
        "outputId": "153eb529-4bfc-4b00-c3fa-db7ca24070d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 0s 4ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.7206985 , 0.15780483, 0.12149665],\n",
              "       [0.7082935 , 0.15948762, 0.1322188 ],\n",
              "       [0.7201172 , 0.16107269, 0.1188101 ],\n",
              "       [0.689557  , 0.1776471 , 0.13279583],\n",
              "       [0.71994376, 0.16186039, 0.11819573],\n",
              "       [0.67967796, 0.18359064, 0.13673133],\n",
              "       [0.6970984 , 0.18123922, 0.12166246],\n",
              "       [0.7049367 , 0.16620053, 0.12886284],\n",
              "       [0.69503987, 0.1757261 , 0.12923406],\n",
              "       [0.70825577, 0.15948728, 0.13225704],\n",
              "       [0.71807986, 0.15670972, 0.1252103 ],\n",
              "       [0.68534195, 0.1805984 , 0.13405968],\n",
              "       [0.7168443 , 0.15531182, 0.12784377],\n",
              "       [0.74305624, 0.1517786 , 0.10516509],\n",
              "       [0.75497115, 0.13625643, 0.10877245],\n",
              "       [0.72135055, 0.16457309, 0.11407634],\n",
              "       [0.72960985, 0.15812786, 0.11226238],\n",
              "       [0.7124483 , 0.16405974, 0.12349196],\n",
              "       [0.69511724, 0.16607845, 0.13880429],\n",
              "       [0.7042658 , 0.17334957, 0.12238469],\n",
              "       [0.6894015 , 0.16615906, 0.14443941],\n",
              "       [0.6944433 , 0.17907393, 0.12648273],\n",
              "       [0.75657666, 0.15049203, 0.0929313 ],\n",
              "       [0.646051  , 0.19977841, 0.15417054],\n",
              "       [0.6376982 , 0.20526779, 0.15703394],\n",
              "       [0.68515545, 0.16944149, 0.14540309],\n",
              "       [0.67118204, 0.1881977 , 0.14062026],\n",
              "       [0.71101767, 0.16077681, 0.12820557],\n",
              "       [0.7207506 , 0.154123  , 0.1251265 ],\n",
              "       [0.67952305, 0.18242411, 0.13805282],\n",
              "       [0.68140185, 0.17744844, 0.14114963],\n",
              "       [0.69565564, 0.16689546, 0.13744885],\n",
              "       [0.7245613 , 0.16174284, 0.11369587],\n",
              "       [0.73691374, 0.15346873, 0.10961749],\n",
              "       [0.6977217 , 0.16678953, 0.13548869],\n",
              "       [0.7356226 , 0.14687541, 0.11750196],\n",
              "       [0.73573303, 0.14266592, 0.1216011 ],\n",
              "       [0.7258255 , 0.15885994, 0.11531452],\n",
              "       [0.71027905, 0.16932607, 0.12039477],\n",
              "       [0.7072848 , 0.16296636, 0.12974885],\n",
              "       [0.72232   , 0.16099092, 0.11668912],\n",
              "       [0.6850826 , 0.16889168, 0.1460257 ],\n",
              "       [0.71267194, 0.17160165, 0.11572634],\n",
              "       [0.6523075 , 0.20541562, 0.14227685],\n",
              "       [0.6333209 , 0.2123348 , 0.15434426],\n",
              "       [0.69586086, 0.17014003, 0.13399906],\n",
              "       [0.69935775, 0.17367579, 0.12696652],\n",
              "       [0.7048217 , 0.17132804, 0.1238502 ],\n",
              "       [0.71608543, 0.15960519, 0.12430934],\n",
              "       [0.71562326, 0.15897064, 0.12540609],\n",
              "       [0.18458098, 0.40409994, 0.41131905],\n",
              "       [0.1663222 , 0.4222945 , 0.41138324],\n",
              "       [0.16024196, 0.41237527, 0.4273827 ],\n",
              "       [0.17900008, 0.41859198, 0.40240788],\n",
              "       [0.16487393, 0.4128351 , 0.42229098],\n",
              "       [0.1516627 , 0.43114716, 0.41719007],\n",
              "       [0.14456074, 0.43392664, 0.4215126 ],\n",
              "       [0.24888714, 0.40494803, 0.3461648 ],\n",
              "       [0.18153962, 0.4040701 , 0.4143902 ],\n",
              "       [0.16619599, 0.446234  , 0.3875701 ],\n",
              "       [0.2314888 , 0.3998428 , 0.3686685 ],\n",
              "       [0.1672664 , 0.4313103 , 0.40142334],\n",
              "       [0.23750804, 0.37516472, 0.3873272 ],\n",
              "       [0.14783527, 0.4254948 , 0.42666987],\n",
              "       [0.22466429, 0.412957  , 0.3623787 ],\n",
              "       [0.19575524, 0.40360025, 0.4006445 ],\n",
              "       [0.13744335, 0.44767916, 0.41487747],\n",
              "       [0.21000154, 0.39991206, 0.39008638],\n",
              "       [0.16140887, 0.40740266, 0.43118846],\n",
              "       [0.21049157, 0.40341333, 0.3860951 ],\n",
              "       [0.11894609, 0.4532118 , 0.42784205],\n",
              "       [0.20989901, 0.4026095 , 0.38749164],\n",
              "       [0.1395344 , 0.41714036, 0.44332528],\n",
              "       [0.16048697, 0.41591746, 0.4235955 ],\n",
              "       [0.19691637, 0.40274248, 0.4003411 ],\n",
              "       [0.19098696, 0.4050906 , 0.4039225 ],\n",
              "       [0.17063662, 0.40406695, 0.4252964 ],\n",
              "       [0.13726474, 0.42256826, 0.44016695],\n",
              "       [0.15035553, 0.43023476, 0.4194097 ],\n",
              "       [0.27506405, 0.37343395, 0.35150203],\n",
              "       [0.21555832, 0.40212387, 0.38231787],\n",
              "       [0.2371633 , 0.39079028, 0.37204644],\n",
              "       [0.21174219, 0.40445167, 0.38380608],\n",
              "       [0.1161618 , 0.4349207 , 0.4489175 ],\n",
              "       [0.13190895, 0.45630938, 0.41178155],\n",
              "       [0.14703417, 0.44437295, 0.4085928 ],\n",
              "       [0.16522898, 0.41458878, 0.42018226],\n",
              "       [0.18815933, 0.39544028, 0.4164002 ],\n",
              "       [0.1757756 , 0.4317019 , 0.39252248],\n",
              "       [0.17860846, 0.4232475 , 0.39814395],\n",
              "       [0.15753345, 0.42927673, 0.4131897 ],\n",
              "       [0.15365593, 0.4260751 , 0.42026892],\n",
              "       [0.2021038 , 0.40590838, 0.3919878 ],\n",
              "       [0.2551067 , 0.39622313, 0.34867015],\n",
              "       [0.16745004, 0.42786825, 0.40468183],\n",
              "       [0.17922671, 0.42428815, 0.39648518],\n",
              "       [0.1718154 , 0.4276366 , 0.4005481 ],\n",
              "       [0.18648109, 0.41020924, 0.4033097 ],\n",
              "       [0.29393086, 0.3834792 , 0.32258987],\n",
              "       [0.17964122, 0.42320985, 0.39714885],\n",
              "       [0.07320876, 0.4573853 , 0.46940595],\n",
              "       [0.09827165, 0.45130688, 0.4504214 ],\n",
              "       [0.09935732, 0.4242997 , 0.4763429 ],\n",
              "       [0.09998506, 0.43548167, 0.46453324],\n",
              "       [0.08621978, 0.44024613, 0.4735341 ],\n",
              "       [0.09343105, 0.41537425, 0.4911946 ],\n",
              "       [0.10461779, 0.48168123, 0.41370088],\n",
              "       [0.10353622, 0.414523  , 0.48194078],\n",
              "       [0.10264201, 0.42002952, 0.47732854],\n",
              "       [0.08840373, 0.43658727, 0.4750091 ],\n",
              "       [0.11475304, 0.44003856, 0.44520834],\n",
              "       [0.1066611 , 0.43162295, 0.46171597],\n",
              "       [0.1040428 , 0.43093753, 0.4650195 ],\n",
              "       [0.09374059, 0.45527482, 0.4509846 ],\n",
              "       [0.0820479 , 0.47021893, 0.4477331 ],\n",
              "       [0.09561863, 0.45060763, 0.4537737 ],\n",
              "       [0.10742703, 0.43162814, 0.46094498],\n",
              "       [0.09300428, 0.42417824, 0.48281747],\n",
              "       [0.08277369, 0.41219714, 0.50502914],\n",
              "       [0.12529325, 0.42241856, 0.45228824],\n",
              "       [0.09509185, 0.43537995, 0.4695282 ],\n",
              "       [0.09697226, 0.4647189 , 0.43830878],\n",
              "       [0.09619544, 0.4101648 , 0.4936398 ],\n",
              "       [0.12400237, 0.43022913, 0.4457684 ],\n",
              "       [0.09733662, 0.43752983, 0.46513358],\n",
              "       [0.10892487, 0.4205168 , 0.47055826],\n",
              "       [0.12559244, 0.43480107, 0.43960643],\n",
              "       [0.1187752 , 0.44238424, 0.4388405 ],\n",
              "       [0.09093428, 0.43832657, 0.47073928],\n",
              "       [0.12267654, 0.41390207, 0.46342137],\n",
              "       [0.10745571, 0.41310984, 0.4794344 ],\n",
              "       [0.10716088, 0.42171964, 0.47111958],\n",
              "       [0.0876755 , 0.44072804, 0.4715964 ],\n",
              "       [0.12937787, 0.42460066, 0.44602147],\n",
              "       [0.1130091 , 0.42719823, 0.45979252],\n",
              "       [0.10215191, 0.41714922, 0.48069885],\n",
              "       [0.08476799, 0.4585733 , 0.45665866],\n",
              "       [0.10560012, 0.4357081 , 0.4586919 ],\n",
              "       [0.1202595 , 0.44556051, 0.43417996],\n",
              "       [0.1104283 , 0.43039197, 0.45917967],\n",
              "       [0.08999871, 0.44127443, 0.46872687],\n",
              "       [0.11584607, 0.43500635, 0.44914758],\n",
              "       [0.09827165, 0.45130688, 0.4504214 ],\n",
              "       [0.08806706, 0.43696305, 0.47496998],\n",
              "       [0.08617017, 0.4463483 , 0.46748152],\n",
              "       [0.10527439, 0.43834525, 0.45638043],\n",
              "       [0.11477523, 0.4298842 , 0.45534056],\n",
              "       [0.11009576, 0.4363343 , 0.45356998],\n",
              "       [0.09061373, 0.46055076, 0.44883546],\n",
              "       [0.10647288, 0.4494565 , 0.44407058]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## argmax\n",
        "yhat = np.argmax(nn.predict(X), axis = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bQdWMu-mokq",
        "outputId": "0c3b2209-dbd6-4abd-d1e2-a356bdc9213a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, recall_score"
      ],
      "metadata": {
        "id": "Hn2jg-ksmzkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ConfusionMatrixDisplay(confusion_matrix(y, yhat)).plot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "RTfK4K3unkuc",
        "outputId": "31282dcb-14eb-4469-9eca-5dc29ca834d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7e56dfafeb00>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "recall_score(y, yhat, average = \"macro\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfzW5zXinpev",
        "outputId": "98ac959e-fb34-4f61-c73b-cdf21a23dd0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8333333333333334"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "recall_score(y, yhat, average = \"micro\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-J3nSnTtn0FI",
        "outputId": "0011b4fe-4a63-4015-d2e2-b09f010b51bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8333333333333334"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yAtAFD_Rn-DQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}