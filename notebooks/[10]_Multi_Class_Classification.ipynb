{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Multi-Class Classification**\n",
        "\n",
        "We have more than 2 levels. We are using the categorical cross-entropy loss and cost functions."
      ],
      "metadata": {
        "id": "-TJPsGQnhB7V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "DqhnSMgihBRa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "## suppose this is a car\n",
        "y_true = np.array([0,1,0])\n",
        "p_hat = np.array([0.05,0.9,0.00])\n",
        "\n",
        "def categorical_crossentropy(y_true, p_hat):\n",
        "  return -(y_true[0]*np.log(p_hat[0]) + y_true[1]*np.log(p_hat[1]) + y_true[2]*np.log(p_hat[2])) ## 3 classes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## what happens when p_hat is zero?\n",
        "## the p_hats get clipped (+ or - some small error if zero)\n",
        "categorical_crossentropy(y_true, p_hat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFVtoTcehVBk",
        "outputId": "da4d554e-2733-4a09-be84-5814b787b07d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-7314fbd14c7b>:8: RuntimeWarning: divide by zero encountered in log\n",
            "  return -(y_true[0]*np.log(p_hat[0]) + y_true[1]*np.log(p_hat[1]) + y_true[2]*np.log(p_hat[2])) ## 3 classes\n",
            "<ipython-input-22-7314fbd14c7b>:8: RuntimeWarning: invalid value encountered in scalar multiply\n",
            "  return -(y_true[0]*np.log(p_hat[0]) + y_true[1]*np.log(p_hat[1]) + y_true[2]*np.log(p_hat[2])) ## 3 classes\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "nan"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## modified to account for zero\n",
        "def categorical_crossentropy(y_true, p_hat):\n",
        "  epsilon = 1e-15\n",
        "  p_hat = np.clip(p_hat, epsilon, 1-epsilon)\n",
        "  return -np.sum(y_true*np.log(p_hat))"
      ],
      "metadata": {
        "id": "y_NLZA29ifCF"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_crossentropy(y_true, p_hat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FcOUR6xjbWP",
        "outputId": "504d6588-83dd-4a09-fdb3-cc86855e387c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.10536051565782628"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## what is the chance that a p_hat = 0?\n",
        "## statistically, should be 0\n",
        "## does it happen? yes"
      ],
      "metadata": {
        "id": "w1RpIIK5jooY"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## good model\n",
        "y_true = [1,0,0]\n",
        "p_hat =[0.99, 0.005, 0.005]\n",
        "\n",
        "categorical_crossentropy(y_true, p_hat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3D6Xd2MGj6qI",
        "outputId": "540295aa-7ad4-4c3f-92b0-f6f7316ca86c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.01005033585350145"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## good model\n",
        "y_true = [1,0,0]\n",
        "p_hat =[0.05, .7, 0.25]\n",
        "\n",
        "categorical_crossentropy(y_true, p_hat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMQwIoKTkHfo",
        "outputId": "269e05f6-0096-40eb-872f-1bce187f3515"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.995732273553991"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Categorical cross-entropy COST FUNCTION\n",
        "## the average of the loss values across observations\n",
        "## modified to account for zero\n",
        "## if we assume that y_true and p_hat are matrices\n",
        "def categorical_crossentropy_cost(y_true, p_hat):\n",
        "  epsilon = 1e-15\n",
        "  p_hat = np.clip(p_hat, epsilon, 1-epsilon)\n",
        "  return -np.mean(y_true*np.log(p_hat))"
      ],
      "metadata": {
        "id": "wk6EralLkO3h"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Example**\n",
        "\n",
        "Iris dataset. Flowers with measures of the petal and sepal width and length. We want to predict whether they are {setosa, versicolor, virginica}."
      ],
      "metadata": {
        "id": "SrWZXpTTmGbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "X, y = datasets.load_iris(return_X_y = True)"
      ],
      "metadata": {
        "id": "9wFHzj8Ik2uq"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LzAfGl4mxIc",
        "outputId": "5ce52228-dd8e-4e16-9e36-5053e82a2d02"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Most models use One vs All (OvR)\n",
        "## the models get broken down into a series of binary classifications\n",
        "## setosa vs {virginica, versicolor}  we get p_hat1\n",
        "## virginica vs {setosa, versicolor}  we get p_hat2\n",
        "## versicolor vs {setosa, virginical} we get p_hat3\n",
        "## obs1 = softmax([phat1, phat2, phat3]) to sum to 1"
      ],
      "metadata": {
        "id": "9Hs6c0plmyOU"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## A second strategy One vs One (OvO)\n",
        "## the models get broken into series of binary classifications\n",
        "## setosa vs virginica  we get p_hat1\n",
        "## setosa vs versicolor we get p_hat2\n",
        "## virginica vs versicolor we get p_hat3\n",
        "## obs = softmax([phat1, phat2, phat3, ....])"
      ],
      "metadata": {
        "id": "lAnF9XYynrel"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "## instance\n",
        "## this is a ONE VS REST approach\n",
        "## cost function: categorical cross entropy (multinomial logistic regression)\n",
        "## softmax\n",
        "lr = LogisticRegression(max_iter = 500)\n",
        "\n",
        "## fit\n",
        "lr.fit(X,y)\n",
        "\n",
        "## predict\n",
        "yhat = lr.predict(X)"
      ],
      "metadata": {
        "id": "lPRFE8aboJMN"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## if predictive model, no need to interpret\n",
        "## predictive model (descriptive)\n",
        "np.round(lr.predict_proba(X), decimals=2) ## sum to 1 because of the softmax function"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsQf0o6RocMN",
        "outputId": "197cfd46-b818-401f-8ff5-0ae24658429a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.98, 0.02, 0.  ],\n",
              "       [0.97, 0.03, 0.  ],\n",
              "       [0.99, 0.01, 0.  ],\n",
              "       [0.98, 0.02, 0.  ],\n",
              "       [0.99, 0.01, 0.  ],\n",
              "       [0.97, 0.03, 0.  ],\n",
              "       [0.99, 0.01, 0.  ],\n",
              "       [0.98, 0.02, 0.  ],\n",
              "       [0.98, 0.02, 0.  ],\n",
              "       [0.97, 0.03, 0.  ],\n",
              "       [0.98, 0.02, 0.  ],\n",
              "       [0.98, 0.02, 0.  ],\n",
              "       [0.97, 0.03, 0.  ],\n",
              "       [0.99, 0.01, 0.  ],\n",
              "       [0.99, 0.01, 0.  ],\n",
              "       [0.99, 0.01, 0.  ],\n",
              "       [0.99, 0.01, 0.  ],\n",
              "       [0.98, 0.02, 0.  ],\n",
              "       [0.96, 0.04, 0.  ],\n",
              "       [0.98, 0.02, 0.  ],\n",
              "       [0.95, 0.05, 0.  ],\n",
              "       [0.98, 0.02, 0.  ],\n",
              "       [1.  , 0.  , 0.  ],\n",
              "       [0.95, 0.05, 0.  ],\n",
              "       [0.95, 0.05, 0.  ],\n",
              "       [0.95, 0.05, 0.  ],\n",
              "       [0.97, 0.03, 0.  ],\n",
              "       [0.97, 0.03, 0.  ],\n",
              "       [0.98, 0.02, 0.  ],\n",
              "       [0.97, 0.03, 0.  ],\n",
              "       [0.96, 0.04, 0.  ],\n",
              "       [0.96, 0.04, 0.  ],\n",
              "       [0.99, 0.01, 0.  ],\n",
              "       [0.99, 0.01, 0.  ],\n",
              "       [0.97, 0.03, 0.  ],\n",
              "       [0.98, 0.02, 0.  ],\n",
              "       [0.98, 0.02, 0.  ],\n",
              "       [0.99, 0.01, 0.  ],\n",
              "       [0.99, 0.01, 0.  ],\n",
              "       [0.97, 0.03, 0.  ],\n",
              "       [0.99, 0.01, 0.  ],\n",
              "       [0.96, 0.04, 0.  ],\n",
              "       [0.99, 0.01, 0.  ],\n",
              "       [0.97, 0.03, 0.  ],\n",
              "       [0.96, 0.04, 0.  ],\n",
              "       [0.97, 0.03, 0.  ],\n",
              "       [0.98, 0.02, 0.  ],\n",
              "       [0.98, 0.02, 0.  ],\n",
              "       [0.98, 0.02, 0.  ],\n",
              "       [0.98, 0.02, 0.  ],\n",
              "       [0.  , 0.87, 0.12],\n",
              "       [0.01, 0.86, 0.13],\n",
              "       [0.  , 0.73, 0.27],\n",
              "       [0.02, 0.94, 0.04],\n",
              "       [0.  , 0.82, 0.18],\n",
              "       [0.01, 0.86, 0.13],\n",
              "       [0.  , 0.72, 0.28],\n",
              "       [0.15, 0.85, 0.  ],\n",
              "       [0.  , 0.9 , 0.1 ],\n",
              "       [0.04, 0.91, 0.05],\n",
              "       [0.06, 0.94, 0.01],\n",
              "       [0.02, 0.9 , 0.09],\n",
              "       [0.01, 0.98, 0.01],\n",
              "       [0.  , 0.78, 0.22],\n",
              "       [0.07, 0.92, 0.01],\n",
              "       [0.01, 0.93, 0.07],\n",
              "       [0.01, 0.77, 0.22],\n",
              "       [0.02, 0.97, 0.02],\n",
              "       [0.  , 0.8 , 0.2 ],\n",
              "       [0.02, 0.96, 0.02],\n",
              "       [0.  , 0.44, 0.56],\n",
              "       [0.02, 0.96, 0.03],\n",
              "       [0.  , 0.6 , 0.4 ],\n",
              "       [0.  , 0.86, 0.14],\n",
              "       [0.01, 0.94, 0.05],\n",
              "       [0.01, 0.92, 0.07],\n",
              "       [0.  , 0.8 , 0.2 ],\n",
              "       [0.  , 0.48, 0.52],\n",
              "       [0.01, 0.81, 0.18],\n",
              "       [0.06, 0.93, 0.  ],\n",
              "       [0.03, 0.96, 0.01],\n",
              "       [0.04, 0.96, 0.01],\n",
              "       [0.03, 0.96, 0.02],\n",
              "       [0.  , 0.35, 0.65],\n",
              "       [0.01, 0.75, 0.24],\n",
              "       [0.01, 0.79, 0.2 ],\n",
              "       [0.  , 0.81, 0.19],\n",
              "       [0.  , 0.91, 0.08],\n",
              "       [0.03, 0.93, 0.04],\n",
              "       [0.02, 0.94, 0.04],\n",
              "       [0.01, 0.9 , 0.09],\n",
              "       [0.  , 0.83, 0.17],\n",
              "       [0.02, 0.96, 0.03],\n",
              "       [0.12, 0.88, 0.  ],\n",
              "       [0.01, 0.92, 0.07],\n",
              "       [0.02, 0.94, 0.04],\n",
              "       [0.02, 0.93, 0.06],\n",
              "       [0.01, 0.94, 0.06],\n",
              "       [0.24, 0.76, 0.  ],\n",
              "       [0.02, 0.94, 0.04],\n",
              "       [0.  , 0.  , 1.  ],\n",
              "       [0.  , 0.16, 0.84],\n",
              "       [0.  , 0.03, 0.97],\n",
              "       [0.  , 0.08, 0.92],\n",
              "       [0.  , 0.02, 0.98],\n",
              "       [0.  , 0.  , 1.  ],\n",
              "       [0.01, 0.51, 0.48],\n",
              "       [0.  , 0.02, 0.98],\n",
              "       [0.  , 0.05, 0.95],\n",
              "       [0.  , 0.01, 0.99],\n",
              "       [0.  , 0.21, 0.79],\n",
              "       [0.  , 0.14, 0.86],\n",
              "       [0.  , 0.07, 0.93],\n",
              "       [0.  , 0.15, 0.85],\n",
              "       [0.  , 0.04, 0.96],\n",
              "       [0.  , 0.05, 0.95],\n",
              "       [0.  , 0.12, 0.88],\n",
              "       [0.  , 0.  , 1.  ],\n",
              "       [0.  , 0.  , 1.  ],\n",
              "       [0.  , 0.45, 0.55],\n",
              "       [0.  , 0.02, 0.98],\n",
              "       [0.  , 0.19, 0.81],\n",
              "       [0.  , 0.  , 1.  ],\n",
              "       [0.  , 0.39, 0.61],\n",
              "       [0.  , 0.04, 0.96],\n",
              "       [0.  , 0.05, 0.95],\n",
              "       [0.  , 0.46, 0.54],\n",
              "       [0.  , 0.39, 0.61],\n",
              "       [0.  , 0.04, 0.96],\n",
              "       [0.  , 0.14, 0.86],\n",
              "       [0.  , 0.03, 0.97],\n",
              "       [0.  , 0.02, 0.98],\n",
              "       [0.  , 0.03, 0.97],\n",
              "       [0.  , 0.48, 0.52],\n",
              "       [0.  , 0.19, 0.81],\n",
              "       [0.  , 0.01, 0.99],\n",
              "       [0.  , 0.02, 0.98],\n",
              "       [0.  , 0.12, 0.88],\n",
              "       [0.  , 0.44, 0.56],\n",
              "       [0.  , 0.09, 0.91],\n",
              "       [0.  , 0.02, 0.98],\n",
              "       [0.  , 0.12, 0.88],\n",
              "       [0.  , 0.16, 0.84],\n",
              "       [0.  , 0.01, 0.99],\n",
              "       [0.  , 0.01, 0.99],\n",
              "       [0.  , 0.08, 0.92],\n",
              "       [0.  , 0.25, 0.75],\n",
              "       [0.  , 0.16, 0.84],\n",
              "       [0.  , 0.04, 0.96],\n",
              "       [0.  , 0.23, 0.76]])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## prediction is np.argmax(p_hat, axis = 1)\n",
        "p_hat = lr.predict_proba(X)\n",
        "np.argmax(p_hat, axis = 1) ## select the highest prob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5DmyBEaogIl",
        "outputId": "d025bef1-9278-4418-ba84-5ec04152e0ea"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## interpretations\n",
        "lr.coef_  ## slopes of each variable\n",
        "## model 1: class 0 vs the rest (-0.42456599,  0.96664261, -2.51554625, -1.08216927)\n",
        "## x1 = sepal width\n",
        "## model 1: as x1 increases by 1, then the log(odds) of being a setosa flower decrease 0.42456599"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRN7UiqiqN02",
        "outputId": "9d9eb43e-59d2-4914-b33f-dbdab080bd44"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.42456599,  0.96664261, -2.51554625, -1.08216927],\n",
              "       [ 0.53541119, -0.32073935, -0.20740629, -0.94263206],\n",
              "       [-0.1108452 , -0.64590325,  2.72295254,  2.02480133]])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aUtBn5Bqbm2",
        "outputId": "25bea5b3-9b23-4cd7-b4ed-fd6dfeb0fdfd"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.1, 3.5, 1.4, 0.2],\n",
              "       [4.9, 3. , 1.4, 0.2],\n",
              "       [4.7, 3.2, 1.3, 0.2],\n",
              "       [4.6, 3.1, 1.5, 0.2],\n",
              "       [5. , 3.6, 1.4, 0.2],\n",
              "       [5.4, 3.9, 1.7, 0.4],\n",
              "       [4.6, 3.4, 1.4, 0.3],\n",
              "       [5. , 3.4, 1.5, 0.2],\n",
              "       [4.4, 2.9, 1.4, 0.2],\n",
              "       [4.9, 3.1, 1.5, 0.1],\n",
              "       [5.4, 3.7, 1.5, 0.2],\n",
              "       [4.8, 3.4, 1.6, 0.2],\n",
              "       [4.8, 3. , 1.4, 0.1],\n",
              "       [4.3, 3. , 1.1, 0.1],\n",
              "       [5.8, 4. , 1.2, 0.2],\n",
              "       [5.7, 4.4, 1.5, 0.4],\n",
              "       [5.4, 3.9, 1.3, 0.4],\n",
              "       [5.1, 3.5, 1.4, 0.3],\n",
              "       [5.7, 3.8, 1.7, 0.3],\n",
              "       [5.1, 3.8, 1.5, 0.3],\n",
              "       [5.4, 3.4, 1.7, 0.2],\n",
              "       [5.1, 3.7, 1.5, 0.4],\n",
              "       [4.6, 3.6, 1. , 0.2],\n",
              "       [5.1, 3.3, 1.7, 0.5],\n",
              "       [4.8, 3.4, 1.9, 0.2],\n",
              "       [5. , 3. , 1.6, 0.2],\n",
              "       [5. , 3.4, 1.6, 0.4],\n",
              "       [5.2, 3.5, 1.5, 0.2],\n",
              "       [5.2, 3.4, 1.4, 0.2],\n",
              "       [4.7, 3.2, 1.6, 0.2],\n",
              "       [4.8, 3.1, 1.6, 0.2],\n",
              "       [5.4, 3.4, 1.5, 0.4],\n",
              "       [5.2, 4.1, 1.5, 0.1],\n",
              "       [5.5, 4.2, 1.4, 0.2],\n",
              "       [4.9, 3.1, 1.5, 0.2],\n",
              "       [5. , 3.2, 1.2, 0.2],\n",
              "       [5.5, 3.5, 1.3, 0.2],\n",
              "       [4.9, 3.6, 1.4, 0.1],\n",
              "       [4.4, 3. , 1.3, 0.2],\n",
              "       [5.1, 3.4, 1.5, 0.2],\n",
              "       [5. , 3.5, 1.3, 0.3],\n",
              "       [4.5, 2.3, 1.3, 0.3],\n",
              "       [4.4, 3.2, 1.3, 0.2],\n",
              "       [5. , 3.5, 1.6, 0.6],\n",
              "       [5.1, 3.8, 1.9, 0.4],\n",
              "       [4.8, 3. , 1.4, 0.3],\n",
              "       [5.1, 3.8, 1.6, 0.2],\n",
              "       [4.6, 3.2, 1.4, 0.2],\n",
              "       [5.3, 3.7, 1.5, 0.2],\n",
              "       [5. , 3.3, 1.4, 0.2],\n",
              "       [7. , 3.2, 4.7, 1.4],\n",
              "       [6.4, 3.2, 4.5, 1.5],\n",
              "       [6.9, 3.1, 4.9, 1.5],\n",
              "       [5.5, 2.3, 4. , 1.3],\n",
              "       [6.5, 2.8, 4.6, 1.5],\n",
              "       [5.7, 2.8, 4.5, 1.3],\n",
              "       [6.3, 3.3, 4.7, 1.6],\n",
              "       [4.9, 2.4, 3.3, 1. ],\n",
              "       [6.6, 2.9, 4.6, 1.3],\n",
              "       [5.2, 2.7, 3.9, 1.4],\n",
              "       [5. , 2. , 3.5, 1. ],\n",
              "       [5.9, 3. , 4.2, 1.5],\n",
              "       [6. , 2.2, 4. , 1. ],\n",
              "       [6.1, 2.9, 4.7, 1.4],\n",
              "       [5.6, 2.9, 3.6, 1.3],\n",
              "       [6.7, 3.1, 4.4, 1.4],\n",
              "       [5.6, 3. , 4.5, 1.5],\n",
              "       [5.8, 2.7, 4.1, 1. ],\n",
              "       [6.2, 2.2, 4.5, 1.5],\n",
              "       [5.6, 2.5, 3.9, 1.1],\n",
              "       [5.9, 3.2, 4.8, 1.8],\n",
              "       [6.1, 2.8, 4. , 1.3],\n",
              "       [6.3, 2.5, 4.9, 1.5],\n",
              "       [6.1, 2.8, 4.7, 1.2],\n",
              "       [6.4, 2.9, 4.3, 1.3],\n",
              "       [6.6, 3. , 4.4, 1.4],\n",
              "       [6.8, 2.8, 4.8, 1.4],\n",
              "       [6.7, 3. , 5. , 1.7],\n",
              "       [6. , 2.9, 4.5, 1.5],\n",
              "       [5.7, 2.6, 3.5, 1. ],\n",
              "       [5.5, 2.4, 3.8, 1.1],\n",
              "       [5.5, 2.4, 3.7, 1. ],\n",
              "       [5.8, 2.7, 3.9, 1.2],\n",
              "       [6. , 2.7, 5.1, 1.6],\n",
              "       [5.4, 3. , 4.5, 1.5],\n",
              "       [6. , 3.4, 4.5, 1.6],\n",
              "       [6.7, 3.1, 4.7, 1.5],\n",
              "       [6.3, 2.3, 4.4, 1.3],\n",
              "       [5.6, 3. , 4.1, 1.3],\n",
              "       [5.5, 2.5, 4. , 1.3],\n",
              "       [5.5, 2.6, 4.4, 1.2],\n",
              "       [6.1, 3. , 4.6, 1.4],\n",
              "       [5.8, 2.6, 4. , 1.2],\n",
              "       [5. , 2.3, 3.3, 1. ],\n",
              "       [5.6, 2.7, 4.2, 1.3],\n",
              "       [5.7, 3. , 4.2, 1.2],\n",
              "       [5.7, 2.9, 4.2, 1.3],\n",
              "       [6.2, 2.9, 4.3, 1.3],\n",
              "       [5.1, 2.5, 3. , 1.1],\n",
              "       [5.7, 2.8, 4.1, 1.3],\n",
              "       [6.3, 3.3, 6. , 2.5],\n",
              "       [5.8, 2.7, 5.1, 1.9],\n",
              "       [7.1, 3. , 5.9, 2.1],\n",
              "       [6.3, 2.9, 5.6, 1.8],\n",
              "       [6.5, 3. , 5.8, 2.2],\n",
              "       [7.6, 3. , 6.6, 2.1],\n",
              "       [4.9, 2.5, 4.5, 1.7],\n",
              "       [7.3, 2.9, 6.3, 1.8],\n",
              "       [6.7, 2.5, 5.8, 1.8],\n",
              "       [7.2, 3.6, 6.1, 2.5],\n",
              "       [6.5, 3.2, 5.1, 2. ],\n",
              "       [6.4, 2.7, 5.3, 1.9],\n",
              "       [6.8, 3. , 5.5, 2.1],\n",
              "       [5.7, 2.5, 5. , 2. ],\n",
              "       [5.8, 2.8, 5.1, 2.4],\n",
              "       [6.4, 3.2, 5.3, 2.3],\n",
              "       [6.5, 3. , 5.5, 1.8],\n",
              "       [7.7, 3.8, 6.7, 2.2],\n",
              "       [7.7, 2.6, 6.9, 2.3],\n",
              "       [6. , 2.2, 5. , 1.5],\n",
              "       [6.9, 3.2, 5.7, 2.3],\n",
              "       [5.6, 2.8, 4.9, 2. ],\n",
              "       [7.7, 2.8, 6.7, 2. ],\n",
              "       [6.3, 2.7, 4.9, 1.8],\n",
              "       [6.7, 3.3, 5.7, 2.1],\n",
              "       [7.2, 3.2, 6. , 1.8],\n",
              "       [6.2, 2.8, 4.8, 1.8],\n",
              "       [6.1, 3. , 4.9, 1.8],\n",
              "       [6.4, 2.8, 5.6, 2.1],\n",
              "       [7.2, 3. , 5.8, 1.6],\n",
              "       [7.4, 2.8, 6.1, 1.9],\n",
              "       [7.9, 3.8, 6.4, 2. ],\n",
              "       [6.4, 2.8, 5.6, 2.2],\n",
              "       [6.3, 2.8, 5.1, 1.5],\n",
              "       [6.1, 2.6, 5.6, 1.4],\n",
              "       [7.7, 3. , 6.1, 2.3],\n",
              "       [6.3, 3.4, 5.6, 2.4],\n",
              "       [6.4, 3.1, 5.5, 1.8],\n",
              "       [6. , 3. , 4.8, 1.8],\n",
              "       [6.9, 3.1, 5.4, 2.1],\n",
              "       [6.7, 3.1, 5.6, 2.4],\n",
              "       [6.9, 3.1, 5.1, 2.3],\n",
              "       [5.8, 2.7, 5.1, 1.9],\n",
              "       [6.8, 3.2, 5.9, 2.3],\n",
              "       [6.7, 3.3, 5.7, 2.5],\n",
              "       [6.7, 3. , 5.2, 2.3],\n",
              "       [6.3, 2.5, 5. , 1.9],\n",
              "       [6.5, 3. , 5.2, 2. ],\n",
              "       [6.2, 3.4, 5.4, 2.3],\n",
              "       [5.9, 3. , 5.1, 1.8]])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## do all models have OvR approach?\n",
        "## tree-based (RF,  XGBoost, Adaboost)\n",
        "## does not need to be binary"
      ],
      "metadata": {
        "id": "OmwXB5sXqfOm"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "## instance\n",
        "rf = RandomForestClassifier()\n",
        "\n",
        "## fit and predict\n",
        "yhat1 = rf.fit(X,y).predict(X)"
      ],
      "metadata": {
        "id": "Iud8ZcA-riCn"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yhat1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3Bjcz3crtC_",
        "outputId": "aa8a8246-5db6-435c-9489-8d67823484ee"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## what if you want to implement OvR on a model?\n",
        "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
        "\n",
        "## instance of OnevsRest wrapped around the RF (model)\n",
        "rf_OvR = OneVsRestClassifier(RandomForestClassifier())\n",
        "\n",
        "## fit\n",
        "yhat2 = rf_OvR.fit(X, y).predict(X)"
      ],
      "metadata": {
        "id": "8XBoYcSmrtsv"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yhat2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDdfkz4bshpo",
        "outputId": "4637ec63-3a68-46dc-a020-48b2b9e457fc"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## One vs One might pick difference between similar classes\n",
        "## One vs Rest might be able to isolate some classes that are difficult to predict\n",
        "## if your classes are imbalanced\n",
        "## y [0.45, 0.20, 0.35]"
      ],
      "metadata": {
        "id": "4n0MJFcUsiro"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## metrics\n",
        "## same\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
        "\n",
        "ConfusionMatrixDisplay(confusion_matrix(y, yhat1)).plot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "WVt7mypAtSCA",
        "outputId": "64a6c6ba-5ffe-421c-804a-af6b7f7941e1"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7e66f7517610>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y, yhat1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaslzLMetckA",
        "outputId": "fd631b7f-dfc4-4367-aa17-4dcbecb52e82"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        50\n",
            "           1       1.00      1.00      1.00        50\n",
            "           2       1.00      1.00      1.00        50\n",
            "\n",
            "    accuracy                           1.00       150\n",
            "   macro avg       1.00      1.00      1.00       150\n",
            "weighted avg       1.00      1.00      1.00       150\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input\n",
        "\n",
        "## y = [\"pos\", \"neg\", \"net\"]   ------ y = [1,2,3,]   ## loss = \"categorical_crossentropy\"\n",
        "## y = onehot encode y = [1,0,0]  [0,1,0]            ## loss = \"sparse_categorical_crossentropy\""
      ],
      "metadata": {
        "id": "-iFHG1gRt6_w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}